
import logging
import re
from typing import Dict, List, Optional, Tuple


class DataCleaner:
    
    def __init__(self):
        # Patterns de nettoyage am√©lior√©s
        self.reference_pattern = re.compile(r'\[\s*(?:note\s*)?\d+\s*\]|\[\d+\]', re.IGNORECASE)
        self.pipe_separator_pattern = re.compile(r'\s*\|\s*')
        self.wiki_link_pattern = re.compile(r'\[\[([^|\]]+\|)?([^\]]+)\]\]')
        self.extra_spaces_pattern = re.compile(r'\s+')
        self.unwanted_chars_pattern = re.compile(r'^[\[\(\)\]\-:\s]+|[\[\(\)\]\-:\s]+$')
        
        # Nouveaux patterns pour d√©tecter les probl√®mes
        self.truncated_pattern = re.compile(r'\.{2,}$')  # D√©tecte ... √† la fin
        self.infobox_pattern = re.compile(r'(InformationSource|BiographicalInformation|HealthEnergy|Sell\s*Price)', re.IGNORECASE)
        self.malformed_desc_pattern = re.compile(r'[A-Z][a-z]+[A-Z][a-z]+[A-Z]')  # CamelCase suspect
        
        # Pr√©fixes √† supprimer des cl√©s d'attributs
        self.key_prefixes_to_remove = [
            'Type', 'Home', 'Race', 'Gender', 'Age', 'Affiliation', 'Occupation', 
            'Hair', 'Eye', 'Weapon', 'Job', 'Status', 'Predecessor', 'First', 'Last',
            'Row \\d+ Info', 'Cell \\d+ Info', 'Block\\d+', 'End.*', 'Image', 'Caption',
            'Color', 'Height', 'Weight', 'Born', 'Death', 'World', 'Ship', 'Voice'
        ]
        
        # Mots-cl√©s pour identifier les r√¥les
        self.role_keywords = [
            'type', 'role', 'class', 'occupation', 'job', 'status', 'position', 'rank'
        ]
        
        # Cat√©gories d'attributs pour l'organisation
        self.attribute_categories = {
            'identity': [
                'name', 'full name', 'real name', 'alias', 'aliases', 'nickname', 
                'species', 'race', 'gender', 'age', 'born', 'birth'
            ],
            'appearance': [
                'hair', 'eye', 'eyes', 'height', 'weight', 'appearance', 'color',
                'skin', 'build', 'style'
            ],
            'origin': [
                'origin', 'homeworld', 'home', 'world', 'birthplace', 'nationality', 
                'region', 'planet', 'dimension', 'realm'
            ],
            'affiliation': [
                'affiliation', 'organization', 'faction', 'team', 'guild', 'clan',
                'army', 'group', 'alliance', 'membership'
            ],
            'status': [
                'status', 'occupation', 'job', 'profession', 'class', 'rank', 
                'position', 'title', 'role'
            ],
            'abilities': [
                'abilities', 'powers', 'skills', 'weapon', 'weapons', 'magic', 
                'special', 'equipment', 'gear', 'items'
            ],
            'relationships': [
                'family', 'relatives', 'parents', 'siblings', 'children', 'friends', 
                'enemies', 'allies', 'spouse', 'partner'
            ],
            'media': [
                'first', 'latest', 'debut', 'appearance', 'games', 'movies', 'series',
                'voice actor', 'actor', 'portrayed'
            ]
        }

    def clean_text_content(self, text: str) -> str:
        """Nettoie le contenu textuel des r√©f√©rences et caract√®res ind√©sirables"""
        if not text:
            return ""
        
        # Supprimer les r√©f√©rences wiki [1], [note 1], etc.
        text = self.reference_pattern.sub('', text)
        
        # Supprimer les liens wiki internes [[link|text]] -> text
        text = self.wiki_link_pattern.sub(r'\\2', text)
        
        # Traitement sp√©cial des s√©parateurs pipe
        text = self._clean_pipe_separators(text)
        
        # Nettoyer les espaces multiples
        text = self.extra_spaces_pattern.sub(' ', text)
        
        # Supprimer les caract√®res ind√©sirables en d√©but et fin
        text = self.unwanted_chars_pattern.sub('', text)
        
        # Nettoyer les parenth√®ses orphelines ou mal ferm√©es
        text = re.sub(r'\\(\\s*\\)', '', text)  # Supprimer les parenth√®ses vides
        text = re.sub(r'\\[\\s*\\]', '', text)  # Supprimer les crochets vides
        
        return text.strip()

    def _clean_pipe_separators(self, text: str) -> str:
        """Nettoie intelligemment les s√©parateurs pipe"""
        # Si le texte contient beaucoup de pipes, c'est probablement du contenu mal format√©
        pipe_count = text.count('|')
        if pipe_count > 3:
            # Remplacer les pipes par des espaces, mais garder la structure logique
            # Exemple: "Human | (enhanced | with | DNA)" -> "Human (enhanced with DNA)"
            
            # Pr√©server les structures entre parenth√®ses et crochets
            parts = []
            current_part = ""
            in_parentheses = 0
            in_brackets = 0
            
            i = 0
            while i < len(text):
                char = text[i]
                
                if char == '(':
                    in_parentheses += 1
                elif char == ')':
                    in_parentheses -= 1
                elif char == '[':
                    in_brackets += 1
                elif char == ']':
                    in_brackets -= 1
                elif char == '|' and in_parentheses == 0 and in_brackets == 0:
                    # Pipe en dehors de parenth√®ses/crochets = s√©parateur principal
                    if current_part.strip():
                        parts.append(current_part.strip())
                    current_part = ""
                    i += 1
                    continue
                elif char == '|' and (in_parentheses > 0 or in_brackets > 0):
                    # Pipe dans des parenth√®ses/crochets = remplacer par espace
                    current_part += ' '
                    i += 1
                    continue
                
                current_part += char
                i += 1
            
            if current_part.strip():
                parts.append(current_part.strip())
            
            # Rejoindre les parties principales avec des espaces
            return ' '.join(parts)
        else:
            # Peu de pipes, remplacer simplement par des espaces
            return self.pipe_separator_pattern.sub(' ', text)

    def clean_attribute_key(self, key: str) -> Optional[str]:
        """Nettoie et normalise les cl√©s d'attributs"""
        if not key:
            return None
        
        cleaned_key = key
        
        # Supprimer les pr√©fixes redondants
        for prefix in self.key_prefixes_to_remove:
            pattern = f'^{prefix}\\\\s*'
            cleaned_key = re.sub(pattern, '', cleaned_key, flags=re.IGNORECASE)
        
        # Nettoyer le texte
        cleaned_key = self.clean_text_content(cleaned_key)
        
        # Normaliser la casse (Title Case)
        if cleaned_key:
            cleaned_key = cleaned_key.title()
        
        # Retourner None si la cl√© est trop courte ou vide
        if not cleaned_key or len(cleaned_key) < 2:
            return None
            
        return cleaned_key

    def clean_attribute_value(self, value: str) -> Optional[str]:
        """Nettoie les valeurs d'attributs avec d√©tection des probl√®mes"""
        if not value:
            return None
        
        # D√©tecter les valeurs tronqu√©es
        if self.truncated_pattern.search(value):
            logging.warning(f"Valeur tronqu√©e d√©tect√©e: {value[:50]}...")
        
        # Nettoyer le contenu textuel
        cleaned_value = self.clean_text_content(value)
        
        # S√©parer les informations multilingues
        cleaned_value = self._separate_multilingual_content(cleaned_value)
        
        # Supprimer les valeurs inutiles
        useless_values = ['n/a', 'na', 'unknown', 'none', 'null', '-', '?', '...']
        if cleaned_value.lower() in useless_values:
            return None
        
        # Retourner None si la valeur est trop courte
        if len(cleaned_value) < 2:
            return None
        
        # Limiter la longueur des valeurs
        if len(cleaned_value) > 200:
            cleaned_value = cleaned_value[:197] + '...'
        
        return cleaned_value

    def _separate_multilingual_content(self, text: str) -> str:
        """S√©pare le contenu multilingue et garde la version principale"""
        # Pattern pour d√©tecter contenu japonais/chinois avec termes techniques
        multilingual_pattern = re.compile(r'([^a-zA-Z0-9\\s\\-\\(\\)]+)\\s*\\(\\s*([^)]+)\\s*\\)', re.UNICODE)
        
        # Si on trouve du contenu multilingue, garder la version entre parenth√®ses (g√©n√©ralement anglais)
        match = multilingual_pattern.search(text)
        if match:
            # Garder la version en parenth√®ses si elle semble √™tre en anglais
            parentheses_content = match.group(2)
            if all(ord(char) < 128 for char in parentheses_content):
                return parentheses_content.strip()
        
        return text

    def clean_attributes_dict(self, attributes: Dict[str, str]) -> Dict[str, str]:
        """Nettoie un dictionnaire d'attributs"""
        cleaned = {}
        
        for key, value in attributes.items():
            # Nettoyer la cl√© et la valeur
            clean_key = self.clean_attribute_key(key)
            clean_value = self.clean_attribute_value(value)
            
            # Ajouter seulement si les deux sont valides
            if clean_key and clean_value:
                cleaned[clean_key] = clean_value
        
        return cleaned

    def extract_enhanced_description(self, soup, infobox) -> str:
        """Extraction am√©lior√©e de description sans contamination d'infobox"""
        descriptions = []
        
        # Obtenir le texte de l'infobox pour √©viter la duplication
        infobox_text = infobox.get_text() if infobox else ""
        infobox_words = set(infobox_text.lower().split()) if infobox_text else set()
        
        # Chercher les descriptions dans diff√©rents endroits
        content_selectors = [
            'div#mw-content-text p',
            'div.mw-parser-output > p',
            'div.WikiaArticle p',
            'div.page-content p'
        ]
        
        for selector in content_selectors:
            paragraphs = soup.select(selector)
            for p in paragraphs[:3]:  # Examiner les 3 premiers paragraphes
                text = p.get_text(strip=True)
                
                # Filtrer les descriptions de faible qualit√©
                if (len(text) > 20 and 
                    not self._is_low_quality_description(text) and
                    not self._is_infobox_contaminated(text, infobox_words)):
                    descriptions.append(text)
        
        if descriptions:
            # Prendre la meilleure description
            best_desc = max(descriptions, key=len)
            
            # Nettoyer la description
            best_desc = self.clean_text_content(best_desc)
            
            # D√©tecter et signaler les descriptions malform√©es
            if self.infobox_pattern.search(best_desc) or self.malformed_desc_pattern.search(best_desc):
                logging.warning(f"Description malform√©e d√©tect√©e: {best_desc[:100]}...")
                # Essayer de nettoyer automatiquement
                best_desc = self._clean_malformed_description(best_desc)
            
            # Limiter la longueur
            if len(best_desc) > 500:
                best_desc = best_desc[:497] + '...'
            
            return best_desc
        
        return "Description non trouv√©e."

    def _is_infobox_contaminated(self, text: str, infobox_words: set) -> bool:
        """V√©rifie si le texte contient trop de mots de l'infobox"""
        if not infobox_words:
            return False
        
        text_words = set(text.lower().split())
        overlap = len(text_words.intersection(infobox_words))
        return overlap > len(text_words) * 0.5  # Plus de 50% de chevauchement
    
    def _clean_malformed_description(self, text: str) -> str:
        """Nettoie les descriptions malform√©es"""
        # Supprimer les patterns d'infobox coll√©s
        text = self.infobox_pattern.sub('', text)
        
        # Supprimer les mots en CamelCase suspects
        text = re.sub(r'\\b[A-Z][a-z]+[A-Z][a-z]+\\b', '', text)
        
        # Nettoyer les espaces multiples
        text = self.extra_spaces_pattern.sub(' ', text)
        
        return text.strip()

    def _is_low_quality_description(self, desc: str) -> bool:
        """D√©termine si une description est de faible qualit√©"""
        low_quality_indicators = [
            'redirect', 'disambig', 'stub', 'category:', 'template:',
            'see also', 'main article:', 'this article', 'this page',
            'biography information', 'physical description', 'gameplay details'
        ]
        
        desc_lower = desc.lower()
        return any(indicator in desc_lower for indicator in low_quality_indicators)

    def extract_smart_role(self, attributes: Dict[str, str], soup) -> str:
        """Extraction intelligente du r√¥le avec de meilleures priorit√©s"""
        # 1. Chercher dans les attributs nettoy√©s avec priorit√©s sp√©cifiques
        priority_keys = ['position', 'title', 'occupation', 'profession', 'class', 'rank', 'status']
        for priority_key in priority_keys:
            for attr_key, attr_value in attributes.items():
                if priority_key in attr_key.lower() and attr_value:
                    clean_role = self.clean_attribute_value(attr_value)
                    if clean_role and len(clean_role) > 2 and clean_role not in ['Characters', 'Hero', 'Personnage']:
                        return clean_role
        
        # 2. Analyser le contexte de la page pour des r√¥les sp√©cifiques
        page_title = soup.find('h1', id='firstHeading')
        if page_title:
            title_text = page_title.get_text().lower()
            
            # R√¥les sp√©cifiques par domaine
            specific_roles = {
                'games': ['player character', 'npc', 'boss', 'antagonist', 'protagonist'],
                'anime': ['main character', 'supporting character', 'villain', 'hero'],
                'tv': ['main cast', 'recurring character', 'guest star'],
                'movies': ['lead actor', 'supporting actor', 'director']
            }
            
            for domain, roles in specific_roles.items():
                for role in roles:
                    if role in title_text:
                        return role.title()
        
        # 3. Chercher dans les cat√©gories de la page
        categories = soup.find_all('a', href=re.compile(r'/wiki/Category:'))
        for cat in categories:
            cat_text = cat.get_text(strip=True).lower()
            if any(word in cat_text for word in ['character', 'protagonist', 'antagonist', 'villain', 'hero']):
                # Extraire le r√¥le sp√©cifique de la cat√©gorie
                if 'main' in cat_text:
                    return "Main Character"
                elif 'supporting' in cat_text:
                    return "Supporting Character"
                elif 'villain' in cat_text:
                    return "Villain"
                elif 'hero' in cat_text:
                    return "Hero"
        
        # 4. R√¥le par d√©faut plus sp√©cifique selon le fandom
        fandom_type = self._detect_fandom_type(soup)
        if fandom_type == 'game':
            return "Game Character"
        elif fandom_type == 'anime':
            return "Anime Character"
        elif fandom_type == 'tv':
            return "TV Character"
        else:
            return "Character"
    
    def _detect_fandom_type(self, soup) -> str:
        """D√©tecte le type de fandom bas√© sur des indices dans la page"""
        # Analyser l'URL et le contenu pour d√©tecter le type
        page_content = soup.get_text().lower()
        
        if any(word in page_content for word in ['gameplay', 'playable', 'game mechanics', 'level']):
            return 'game'
        elif any(word in page_content for word in ['anime', 'manga', 'episode', 'chapter']):
            return 'anime'
        elif any(word in page_content for word in ['season', 'episode', 'aired', 'television']):
            return 'tv'
        else:
            return 'general'

    def organize_attributes_by_category(self, attributes: Dict[str, str]) -> Dict[str, Dict[str, str]]:
        """Organise les attributs par cat√©gories logiques"""
        organized = {category: {} for category in self.attribute_categories.keys()}
        organized['other'] = {}
        
        for key, value in attributes.items():
            categorized = False
            key_lower = key.lower()
            
            # Chercher dans quelle cat√©gorie placer cet attribut
            for category, keywords in self.attribute_categories.items():
                if any(keyword in key_lower for keyword in keywords):
                    organized[category][key] = value
                    categorized = True
                    break
            
            # Si pas de cat√©gorie trouv√©e, mettre dans 'other'
            if not categorized:
                organized['other'][key] = value
        
        # Supprimer les cat√©gories vides
        return {k: v for k, v in organized.items() if v}

    def validate_and_clean_result(self, result: Dict) -> Optional[Dict]:
        """Valide et nettoie un r√©sultat final avec d√©tection avanc√©e des probl√®mes"""
        if not result:
            return None
        
        # Nettoyer les attributs et d√©tecter les duplications
        attributes = result.get('attributes', {})
        if attributes:
            cleaned_attributes = self.clean_attributes_dict(attributes)
            
            # D√©tecter et supprimer les informations dupliqu√©es avec la description
            description = result.get('description', '')
            if description:
                cleaned_attributes = self._remove_duplicate_information(cleaned_attributes, description)
            
            result['attributes'] = cleaned_attributes
        else:
            result['attributes'] = {}
        
        # Nettoyer la description et d√©tecter les probl√®mes
        description = result.get('description', '')
        if description:
            # D√©tecter infobox contamin√©e
            if self.infobox_pattern.search(description):
                logging.warning(f"Description contamin√©e d√©tect√©e pour: {result.get('name', 'Unknown')}")
                description = self._clean_malformed_description(description)
            
            description = self.clean_text_content(description)
            if len(description) > 500:
                description = description[:497] + '...'
            result['description'] = description
        
        # Nettoyer le r√¥le
        role = result.get('role', '')
        if role:
            role = self.clean_text_content(role)
            result['role'] = role
        
        # Valider la qualit√© finale
        quality_score = self._calculate_quality_score(result)
        if quality_score < 3:  # Score minimum requis
            logging.info(f"R√©sultat rejet√© pour qualit√© insuffisante: {result.get('name', 'Unknown')} (score: {quality_score})")
            return None
        
        return result
    
    def _remove_duplicate_information(self, attributes: Dict[str, str], description: str) -> Dict[str, str]:
        """Supprime les informations dupliqu√©es entre attributs et description"""
        if not description:
            return attributes
        
        desc_lower = description.lower()
        cleaned_attributes = {}
        
        for key, value in attributes.items():
            # V√©rifier si la valeur de l'attribut est pr√©sente dans la description
            value_lower = str(value).lower()
            
            # Seuil de similarit√© pour √©viter les faux positifs
            if len(value_lower) > 10 and value_lower in desc_lower:
                logging.debug(f"Information dupliqu√©e supprim√©e: {key} = {value}")
                continue
            
            cleaned_attributes[key] = value
        
        return cleaned_attributes

    def _calculate_quality_score(self, result: Dict) -> int:
        """Calcule un score de qualit√© pour un r√©sultat"""
        score = 0
        
        # Nom valide (+1)
        if result.get('name') and len(result['name']) > 2:
            score += 1
        
        # Image valide (+2)
        if result.get('image_url') and result['image_url'].startswith('http'):
            score += 2
        
        # Description pertinente (+2)
        desc = result.get('description', '')
        if desc and desc != 'Description non trouv√©e.' and len(desc) > 50:
            score += 2
        
        # R√¥le d√©fini et sp√©cifique (+1)
        role = result.get('role', '')
        if role and role not in ['N/A', 'Personnage', 'Characters', 'Hero']:
            score += 1
        
        # Attributs (+1 par attribut, max 3)
        attributes = result.get('attributes', {})
        score += min(len(attributes), 3)
        
        return score

    def generate_quality_statistics(self, results: List[Dict]) -> str:
        """G√©n√®re des statistiques d√©taill√©es sur la qualit√© des donn√©es"""
        if not results:
            return "Aucune donn√©e √† analyser."
        
        total = len(results)
        
        # Statistiques de base
        with_description = sum(1 for r in results 
                             if r.get('description') and 
                             r['description'] != 'Description non trouv√©e.' and 
                             len(r['description']) > 50)
        
        with_specific_role = sum(1 for r in results 
                               if r.get('role') and 
                               r['role'] not in ['N/A', 'Personnage', 'Characters', 'Hero'])
        
        with_attributes = sum(1 for r in results if r.get('attributes'))
        
        avg_attributes = sum(len(r.get('attributes', {})) for r in results) / total
        
        # Scores de qualit√©
        quality_scores = [self._calculate_quality_score(r) for r in results]
        avg_quality = sum(quality_scores) / total
        high_quality = sum(1 for score in quality_scores if score >= 7)
        
        return f"""## üìä Statistiques de qualit√© des donn√©es

**Qualit√© g√©n√©rale :**
- üéØ Score de qualit√© moyen : {avg_quality:.1f}/10
- ‚≠ê Fiches haute qualit√© (‚â•7/10) : {high_quality}/{total} ({high_quality/total*100:.1f}%)

**Compl√©tude des donn√©es :**
- üìù Avec description valide : {with_description}/{total} ({with_description/total*100:.1f}%)
- üè∑Ô∏è Avec r√¥le sp√©cifique : {with_specific_role}/{total} ({with_specific_role/total*100:.1f}%)
- üîñ Avec attributs : {with_attributes}/{total} ({with_attributes/total*100:.1f}%)
- üìà Nombre moyen d'attributs : {avg_attributes:.1f}

**Syst√®me de nettoyage :** ‚úÖ Am√©liorations activ√©es
- D√©tection de valeurs tronqu√©es
- S√©paration du contenu multilingue  
- Suppression des duplications
- Validation de qualit√© renforc√©e"""


# Instance globale pour utilisation dans l'application
data_cleaner = DataCleaner()
